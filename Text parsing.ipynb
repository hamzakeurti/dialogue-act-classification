{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.core\n",
    "\n",
    "def parse_dadb(filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    frames = []\n",
    "    \n",
    "    with open(filename,'r') as file:\n",
    "        line = file.readline()\n",
    "        k=0\n",
    "        while line:\n",
    "            line_split = line.split(',')\n",
    "            begin_time,end_time = float(line_split[0]),float(line_split[1])\n",
    "            label = line_split[-9]\n",
    "            words = [x.split('+')[-1] for x in line_split[4].split('|')]\n",
    "\n",
    "\n",
    "            data.append(words)\n",
    "            labels.append(label)\n",
    "            frames.append(librosa.core.time_to_frames((begin_time,end_time)))\n",
    "\n",
    "            line = file.readline()\n",
    "    return data,labels,frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'%', 'b', 'f', 'q', 's', 'z'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapping_labels(label):\n",
    "    if '|' in label:\n",
    "        for i in range(len(label)):\n",
    "            if label[i]=='|':\n",
    "                return label[i+1]\n",
    "    if label=='' or label[0].isdigit():\n",
    "        return 'z'\n",
    "    if label[0] in ['s','q','b','%','f']:\n",
    "        return label[0]\n",
    "    if label[0] == 'h':\n",
    "        return 'f'\n",
    "    if label[0] == 'x':\n",
    "        return '%'\n",
    "    return label\n",
    "set([mapping_labels(label) for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "dev_set = []\n",
    "test_set = []\n",
    "with open('train_test_split.txt','r') as file:\n",
    "    line = file.readline()\n",
    "    train = True\n",
    "    line = file.readline()\n",
    "    while line[:3]!='Dev':\n",
    "        if len(line)>1:\n",
    "            training_set.append(line[:-1])\n",
    "        line = file.readline()\n",
    "    line = file.readline()\n",
    "    while line[:3]!='Tes':\n",
    "        if len(line)>1:\n",
    "            dev_set.append(line[:-1])\n",
    "        line = file.readline()\n",
    "    line = file.readline()\n",
    "    while line[:3]!='Not':\n",
    "        if len(line)>1:\n",
    "            test_set.append(line[:-1])\n",
    "        line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {'train':training_set,'dev':dev_set,'test':test_set}\n",
    "folders = ['train','dev','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(folders,s):\n",
    "    frames_dict = {}\n",
    "    data_dict = {}\n",
    "    labels_dict = {}\n",
    "    vocabulary = {}\n",
    "    dict_index = 1\n",
    "    for state in folders:\n",
    "        for name in s[state]:\n",
    "            data,labels,frames = parse_dadb('data\\\\dadb\\\\' + name + '.dadb')           \n",
    "            sent_len = max(sent_len,max_sent_len)\n",
    "            frames_len = max(frames_len,max_frames_len)\n",
    "            \n",
    "            for x in frames:\n",
    "                lengths.append(x[1]-x[0]) \n",
    "            #update dictionary\n",
    "            for sentence in data:\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        vocabulary[word]\n",
    "                    except:\n",
    "                        vocabulary[word] = dict_index\n",
    "                        dict_index+=1\n",
    "                        \n",
    "            indexed_data = [[vocabulary[word] for word in sentence] for sentence in data]\n",
    "            indexed_data = [[],[]] + indexed_data\n",
    "            data_dict[name] = [[indexed_data[k],indexed_data[k+1],indexed_data[k+2]] for k in range(len(data))]\n",
    "            labels_dict[name] = [mapping_labels(label) for label in labels]\n",
    "            frames_dict[name] = frames\n",
    "            \n",
    "    return vocabulary,data_dict,labels_dict,frames_dict,sent_len,frames_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary,data_dict,labels_dict,frames_dict,sent_len,frames_len = build_vocab(folders,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Bed004'\n",
    "audio = torch.load('data\\\\' + 'train'+'\\\\' + name + '.pt')\n",
    "data = data_dict[name]\n",
    "frames = frames_dict[name]\n",
    "labels = labels_dict[name]\n",
    "sent_len = 50\n",
    "audio_len = 500\n",
    "labels_encoding = {'%':0, 'b':1, 'f':2, 'q':3, 's':4}\n",
    "audio_tensors = []\n",
    "text_tensors = []\n",
    "labels_tensor = []\n",
    "for i in range(len(data)):\n",
    "    begin_f = frames[i][0]\n",
    "    end_f = frames[i][1]\n",
    "    if end_f - begin_f < 501 and labels[i]!='z' and len(data[i]) < 51:\n",
    "        audio_tensors.append(padding(audio[:,begin_f:end_f],500))\n",
    "        text_tensors.append(text_to_torch(data[i],sent_len))\n",
    "        labels_tensor.append(labels_encoding[labels[i]])\n",
    "aud = torch.stack(audio_tensors)\n",
    "text = torch.stack(text_tensors)\n",
    "lab = torch.tensor(labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([817, 3, 78])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_audio(tensor,length):\n",
    "    i,j = tensor.shape\n",
    "    objective = torch.zeros(i,length)\n",
    "    objective[:,:j] = tensor\n",
    "    return objective\n",
    "\n",
    "def text_to_torch(text_list,sent_len):\n",
    "    sentences = [torch.zeros(sent_len) for i in range(3)]\n",
    "    for i in range(3):\n",
    "        text = text_list[i]\n",
    "        sentences[i][:len(text)] = torch.tensor(text)\n",
    "    return torch.stack(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1000e+01, 5.1000e+01, 1.3350e+03, 2.0000e+00, 3.0000e+00, 6.0800e+02,\n",
       "         1.6880e+03, 8.9000e+01, 3.7600e+02, 8.9000e+01, 6.5000e+01, 5.3000e+01,\n",
       "         5.2000e+01, 2.3300e+02, 2.6700e+02, 4.4000e+01, 7.0000e+01, 1.2900e+03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.5300e+02, 3.3000e+01, 7.0000e+01, 2.1210e+03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.7000e+01, 1.8000e+02, 2.2200e+02, 2.9000e+01, 4.3900e+02, 1.8010e+03,\n",
       "         3.3000e+01, 2.2200e+02, 5.3000e+02, 5.0100e+02, 1.3000e+01, 2.1220e+03,\n",
       "         1.3000e+01, 1.8300e+02, 7.0000e+01, 6.4300e+02, 1.6760e+03, 2.7000e+01,\n",
       "         1.6900e+02, 4.4000e+01, 2.1230e+03, 1.2000e+02, 1.1450e+03, 5.9500e+02,\n",
       "         3.3200e+02, 9.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_totorch(final_text[10],78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = final_audio[1].shape\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14599"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['mmm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = [None for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train,train_loader,dataset_dev,dev_loader,dataset_test,test_loader = [None for i in range(6)]\n",
    "variables = {'train':(dataset_train,train_loader),'dev':(dataset_dev,dev_loader),'test':(dataset_test,test_loader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-257-c752f7ca9a02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvariables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "variables['train'][0] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lexical'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-f53389e32eba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatcher\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Documents\\dialogue_act_classification\\dialogue-act-classification\\baseline\\batcher.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlexical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lexical'"
     ]
    }
   ],
   "source": [
    "import baseline.batcher as batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
